{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forest structure using PDAL + Python\n",
    "\n",
    "Dr Adam Steer, November 2019.\n",
    "\n",
    "This work is a set of python modules to replace MATLAB code for generating TERN forest metrics from airborne LIDAR.\n",
    "\n",
    "## Fundamental ideas\n",
    "\n",
    "Existing code uses a series of nested loops, meaning we can't take advantage of array operations or easily reformat or paralellise functionality\n",
    "\n",
    "The approach used here defines a transportable function for each TERN product. These are applied to the data using a single loop (which could be chunked and parallelised).\n",
    "\n",
    "A simple process step-through looks like:\n",
    "\n",
    "1. Read LAS tile using PDAL. This removes an uncompression step. It also removes low outliers and computes normalised height for each point on the fly\n",
    "2. Read numpy labelled arrays from PDAL output into a GeoPandas dataframe, and apply a 2D spatial index\n",
    "3. From LAS file metadata, produce a fishnet grid with cells of size 'output resolution X output resolution'\n",
    "4. Iterate over grid cells, select valid points and generate TERN products for each grid cell\n",
    "5. Assemble an output array for each TERN product and write to GeoTIFF\n",
    "\n",
    "This set of functions operates per-las-tile. An additional layer may be added to merge mutliple raster outputs into larger datasets\n",
    "\n",
    "## to do:\n",
    "\n",
    "- snake_casify variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODATA_VALUE = -9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pdal\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import MultiPolygon\n",
    "from shapely.geometry import box\n",
    "#from shapely.strtree import STRtree\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import osmnx as ox\n",
    "\n",
    "import os\n",
    "\n",
    "# not using this, using geopandas instead\n",
    "from rtree import index\n",
    "\n",
    "# this is needed to create a raster from the output array\n",
    "from osgeo import gdal\n",
    "import osgeo.osr as osr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writegeotiff(griddedpoints, outfile, parameters):\n",
    "    \"\"\"\n",
    "    writes out a geotiff from a numpy array of forest metric\n",
    "    results.\n",
    "    \n",
    "    inputs:\n",
    "    - a numpy array of metrics [griddedpoints]\n",
    "    - an outfile name [outfile]\n",
    "    - a dictionary of parameters for the raster\n",
    "    \n",
    "    outputs:\n",
    "    - a gdal dataset object\n",
    "    - [outfile] written to disk\n",
    "    \"\"\"\n",
    "    \n",
    "    width = parameters[\"width\"]\n",
    "    height = parameters[\"height\"]\n",
    "    \n",
    "    drv = gdal.GetDriverByName(\"GTiff\")\n",
    "    ds = drv.Create(outfile, width, height, 6, gdal.GDT_Float32)\n",
    "    ds.SetGeoTransform(parameters[\"upperleft_y\"],\n",
    "                       parameters[\"resolution\"],\n",
    "                       0,\n",
    "                       parameters[\"upperleft_y\"],\n",
    "                       0,\n",
    "                       parameters[\"resolution\"])\n",
    "    ds.setProjection = parameters[\"projection\"]\n",
    "    ds.GetRasterBand(1).WriteArray(arr)\n",
    "\n",
    "    return(ds)\n",
    "\n",
    "def pdal2df(points):\n",
    "    \"\"\"\n",
    "    Feed me a PDAL pipeline return array, get back a \n",
    "    GeoPandas dataframe \n",
    "    \"\"\"\n",
    "\n",
    "    arr = points[0]\n",
    "    description = arr.dtype.descr\n",
    "    cols = [col for col, __ in description]\n",
    "    gdf = gpd.GeoDataFrame({col: arr[col] for col in cols})\n",
    "    gdf_nodes.name = 'nodes'\n",
    "    gdf_nodes['geometry'] = gdf_nodes.apply(lambda row: Point((row['X'], row['Y'])), axis=1)\n",
    "    \n",
    "    return(gdf_nodes)\n",
    "\n",
    "def spatialindex(dataframe):\n",
    "    sindex = dataframe.sindex\n",
    "    return(sindex)\n",
    "\n",
    "#get a pointview from PDAL\n",
    "def readlasfile(lasfile):\n",
    "    \"\"\"\n",
    "    Run a PDAL pipeline. Input is a JSON declaration to \n",
    "    deliver to PDAL. Output is a labelled numpy array.\n",
    "    \n",
    "    Data are filtered to:\n",
    "    - label local minima as noise\n",
    "    - compute height above ground using nearest ground point\n",
    "      neighbours (TIN method arriving soon)\n",
    "    - sort using a morton order (space filling curve) to \n",
    "      speed indexing later.\n",
    "    \n",
    "    \"\"\"\n",
    "    pipeline = {\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.las\",\n",
    "                \"filename\": lasfile\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.hag\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline))\n",
    "    pipeline.validate()\n",
    "    pipeline.loglevel = 2  # stay quiet\n",
    "    count = pipeline.execute()\n",
    "    \n",
    "    #extract metadata into a JSON blob\n",
    "    metadata = json.loads(pipeline.metadata)\n",
    "    \n",
    "    #read points into labelled arrays\n",
    "    arrays = pipeline.arrays\n",
    "\n",
    "    #return a numpy array to operate on\n",
    "    return(metadata, arrays)\n",
    "\n",
    "def extract_vars(df):\n",
    "    \"\"\"\n",
    "    extract relevant variables\n",
    "    do we need to do this now? or wait till we've grabbed the indexed chunk?\n",
    "    lets write it anyway, then the index chunkifier can call it...\n",
    "    \n",
    "    inputs:\n",
    "    - a numpy labelled array resulting from a PDAL LAS/LAZ file read\n",
    "    \n",
    "    outputs:\n",
    "    - 1D arrays containing relevant variables\n",
    "    \n",
    "    \"\"\"\n",
    "    classification = df[\"Classification\"].values\n",
    "    intensity = df[\"Intensity\"].values\n",
    "    returnnumber = df[\"ReturnNumber\"].values\n",
    "    numberofreturns = df[\"NumberOfReturns\"].values\n",
    "    elevation = df[\"Z\"].values\n",
    "    hag = df[\"HeightAboveGround\"].values\n",
    "\n",
    "    return(intensity, returnnumber, numberofreturns, elevation, hag)\n",
    "\n",
    "\n",
    "def gen_raster_cells(metadata, resolution):\n",
    "    \"\"\"\n",
    "    Generate cells of 'resolution x resolution' for point querying\n",
    "    \n",
    "    input:\n",
    "    - PDAL metadata\n",
    "    \n",
    "    output:\n",
    "    - shapely geometry containing polygons defining 'resolution x resolution'\n",
    "      boxes covering the LAS tile extent\n",
    "      \n",
    "    \"\"\"\n",
    "    bbox = box(metadata[\"metadata\"][\"readers.las\"][0][\"minx\"],\n",
    "               metadata[\"metadata\"][\"readers.las\"][0][\"miny\"],\n",
    "               metadata[\"metadata\"][\"readers.las\"][0][\"maxx\"],\n",
    "               metadata[\"metadata\"][\"readers.las\"][0][\"maxy\"])\n",
    "    \n",
    "    tiledBBox = ox.quadrat_cut_geometry(bbox, quadrat_width=resolution)\n",
    "    \n",
    "    return(tiledBBox)\n",
    "\n",
    "def get_cell_points(poly, df, sindex):\n",
    "    \n",
    "    poly = poly.buffer(1e-14).buffer(0)\n",
    "    possible_matches_index = list(sindex.intersection(poly.bounds))\n",
    "    possible_matches = df.iloc[possible_matches_index]\n",
    "    precise_matches = possible_matches[possible_matches.intersects(poly)]\n",
    "    \n",
    "    return(precise_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vegetation cover fraction: (Nfirst - Nsingle) / Nfirst\n",
    "def comp_vcf(points):\n",
    "    \"\"\"\n",
    "    Computes vegetation cover fraction according to the TERN product manual.\n",
    "    \n",
    "    inputs:\n",
    "    - a labelled array of points from an input LAS tile\n",
    "    \n",
    "    outputs:\n",
    "    - a numpy array of grid cells containing the result of:\n",
    "    \n",
    "    (Nfirst - Nsingle) / Nfirst\n",
    "    \n",
    "    ...where:\n",
    "    Nfirst = count of first returns\n",
    "    Nsingle = count of single returns\n",
    "    \n",
    "    ...per grid cell.\n",
    "    \"\"\"\n",
    "    # collect all the first and single return indices\n",
    "    nSingle = np.size(np.where(points[\"NumberOfReturns\"].values == 1))\n",
    "    nFirst = np.size(np.where(points[\"ReturnNumber\"].values == 1))\n",
    "    if (nFirst > 0):\n",
    "        vcf = (nFirst - nSingle) / nFirst\n",
    "    else:\n",
    "        print('no first returns, set vcf to {}'.format(NODATA_VALUE))\n",
    "        vcf = -9999\n",
    "        \n",
    "    return(vcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canopy layering index:\n",
    "\n",
    "# R = total returns\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vegetation layer cover fraction: LCF\n",
    "\n",
    "def comp_lcf(points, heights, vcf):\n",
    "    \"\"\"\n",
    "    Compute LCF as per the TERN product manual:\n",
    "    \n",
    "    LCF = VCF * (((veg returns below H2) - (veg returns below H1)) / (veg returns below H2))\n",
    "    \n",
    "    Inputs:\n",
    "    - a set of points to compute LCF over\n",
    "    - a height threshold pair, containing H1 and H2 as an array [h1, h2]\n",
    "    - a precomputed VCF\n",
    "    \n",
    "    Outputs:\n",
    "    - a floating point number denoting LCF\n",
    "    \n",
    "    Conditions:\n",
    "    \n",
    "    The LCF *must* be computed over the same set of points as the VCF used as input.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    h1 = heights[0]\n",
    "    h2 = heights[1]\n",
    "    \n",
    "    #find veg returns - ASPRS classes 3,4,5\n",
    "    veg_returns = np.where(np.logical_or(points[\"Classification\"].values == 3,\n",
    "                             points[\"Classification\"].values == 4,\n",
    "                             points[\"Classification\"].values == 5))\n",
    "    # how many veg returns have height below the first threshold?\n",
    "    vegbelowh1 = np.size(np.where(points[\"HeightAboveGround\"][vegreturns] < h1))\n",
    "    \n",
    "    # how many veg returns have height below the second threshold?\n",
    "    vegbelowh2 = np.size(np.where(points[\"HeightAboveGround\"][vegreturns] < h2))\n",
    "    \n",
    "    # compute the LCF\n",
    "    lcf = vcf * ( (vegbelowh2 - vegbelowh1) / vegbelowh2)\n",
    "    \n",
    "    return(lcf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CTH\n",
    "def comp_cth(points):\n",
    "    # compute the highest vegetation point in each grid cell\n",
    "    \n",
    "    veg_returns = np.where(np.logical_or(points[\"Classification\"].values == 3,\n",
    "                             points[\"Classification\"].values == 4,\n",
    "                             points[\"Classification\"].values == 5)) \n",
    "\n",
    "    vegpoints = points[\"HeightAboveGround\"].values[veg_returns]\n",
    "\n",
    "    cth = np.max(vegpoints)\n",
    "\n",
    "    return(cth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_dem(points):\n",
    "    # interpolate ground returns in a grid and output a raster\n",
    "    \n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_fbf(points):\n",
    "    # if building classes exist, compute a fractional conver per grid cell...\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(lasfile):\n",
    "    \"\"\"\n",
    "    wrapper to read in LAS data and produce a dataframe + spatial index\n",
    "    \"\"\"\n",
    "    metadata, points = readlasfile(lasfile)\n",
    "    \n",
    "    dataframe, spatial_index = pdal2df(points)\n",
    "    \n",
    "    return(metadata, dataframe, spatial_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tern_products(metadata, points, sindex, resolution):\n",
    "    \"\"\"\n",
    "    Wrapper to iterate over the input data and generate rasters for each product.\n",
    "    \n",
    "    *note this part could be paralellised - maybe per-product, or per-cell\n",
    "    \n",
    "    Each grid square processed in this loop corresponds to one pixel in an output raster.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #set up an 'output resolution' sized grid - like a fishnet grid.\n",
    "    # each polygon in the resulting set covers an area of 'resolution X resolution'\n",
    "    pixel_grid = gen_raster_cells(metadata, resolution)\n",
    "    \n",
    "    #set up output rasters\n",
    "    \n",
    "    # get tile width and height\n",
    "    tile_width = metadata[\"metadata\"][\"readers.las\"][0][\"maxx\"] - metadata[\"metadata\"][\"readers.las\"][0][\"minx\"]\n",
    "    tile_height = metadata[\"metadata\"][\"readers.las\"][0][\"maxy\"] - metadata[\"metadata\"][\"readers.las\"][0][\"miny\"]\n",
    "\n",
    "    raster_xsize = int(np.ceil(tile_width) / resolution)\n",
    "    raster_ysize = int(np.ceil(tile_height) / resolution)\n",
    "    \n",
    "    print(tile_width)\n",
    "    print(raster_xsize)\n",
    "    \n",
    "    vcf_raster = np.zeros((raster_xsize, raster_ysize))\n",
    "    \n",
    "    print(np.shape(vcf_raster))\n",
    "\n",
    "    lcf_raster = np.zeros((raster_xsize, raster_ysize))\n",
    "    cth_raster = np.zeros((raster_xsize, raster_ysize))\n",
    "    \n",
    "    \n",
    "    for pixel in pixel_grid:\n",
    "        \n",
    "        #compute output array index for this cell:\n",
    "        \n",
    "        poly_x, poly_y = pixel.centroid.xy\n",
    "        \n",
    "        poly_base_x = poly_x[0] - metadata[\"metadata\"][\"readers.las\"][0][\"minx\"]\n",
    "        poly_base_y = poly_y[0] - metadata[\"metadata\"][\"readers.las\"][0][\"miny\"]\n",
    "        \n",
    "        print(poly_base_x)\n",
    "        print(poly_base_y)\n",
    "        \n",
    "        array_x = int(np.floor((poly_base_x / (resolution)) ))\n",
    "        array_y = int(np.floor((poly_base_y / (resolution)) ))\n",
    "        \n",
    "        #print('array X: {}; array Y: {}'.format(array_x, array_y))\n",
    "        \n",
    "        #get points for this cell\n",
    "        matches = get_cell_points(pixel, points, sindex)\n",
    "        \n",
    "        #compute in order\n",
    "        #VCF\n",
    "        vcf_raster[array_x, array_y] = comp_vcf(matches)\n",
    "        \n",
    "        #LCF - need stuff about levels here...\n",
    "        #lcf_raster[array_x, array_y] = comp_lcf(points)\n",
    "        \n",
    "        #CTH\n",
    "        try:\n",
    "            cth_raster[array_x, array_y] = comp_cth(matches)\n",
    "        except ValueError:\n",
    "            print('no vegetation returns were present, CTH set to {} for array index {} {}'.format(NODATA_VALUE, array_x, array_y))\n",
    "            cth_raster[array_x, array_y] = NODATA_VALUE\n",
    "            \n",
    "    #end of computing stuff\n",
    "    \n",
    "    #extract EPSG code from LAS:\n",
    "\n",
    "    \n",
    "    return(vcf_raster, cth_raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing functionality using a local file\n",
    "The following section generates metrics from a local LAZ file. Plugging in download mechanics from ELVIS will be added later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lidar test file - Mt Ainslie, chosen for varied vegetation cover and topography\n",
    "# this is pretty big,\n",
    "\n",
    "lasfile = \"/Volumes/Antares/ACT-lidar/8ppm/callingelvis-testdata/ACT2015_8ppm-C3-AHD_6966094_55.laz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasfile = \"/Volumes/Antares/fire-test/NSW Government - Spatial Services-2/Point Clouds/AHD/StAlbans201709-LID2-C3-AHD_2866308_56_0002_0002/StAlbans201709-LID2-C3-AHD_2866308_56_0002_0002.las\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump everything from memory\n",
    "points = None\n",
    "df = None\n",
    "vcf_raster = None\n",
    "cth_raster = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 30s, sys: 26.2 s, total: 10min 56s\n",
      "Wall time: 11min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# this part of the process is simply reading from the source file. No analysis yet.\n",
    "\n",
    "metadata, points = readlasfile(lasfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#here we read points into a GeoDataFrame and dump the labelled array.\n",
    "# this is a pretty expensive step RAM wise, we're duplicating all the points...\n",
    "\n",
    "df = pdal2df(points)\n",
    "\n",
    "# set points to None, we don't use them anymore\n",
    "points = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# here we generate an RTree index on the dataframe using GeoPandas.\n",
    "# also pretty expensive... \n",
    "\n",
    "sindex = spatialindex(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## rtree index building straight from the point dataset...\n",
    "\n",
    "idx = index.Index()\n",
    "for pid, point in enumerate(points[0]):\n",
    "    idx.insert(pid, (point[0], point[1],point[0], point[1]), point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set an output resolution\n",
    "\n",
    "resolution = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "vcf_raster, cth_raster = compute_tern_products(metadata, df, sindex, resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(vcf_rasters)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cth_rasters)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wktcrs = metadata[\"metadata\"][\"readers.las\"][0][\"comp_spatialreference\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(wktcrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srs = osr.SpatialReference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srs.SetFromUserInput(\"EPSG:28356\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srs.ImportFromWkt(wktcrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srs.GetAuthorityCode(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srs.GetAuthorityName(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiledBBox = gen_raster_cells(metadata,resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiledBBox[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "idxpoints, tree = create_spatial_index(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "thepoints = tree.query(geometry_cut[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a sample workflow on one square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_grid = gen_raster_cells(metadata, resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_grid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = get_cell_points(pixel_grid[10],df, sindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_cth1(points):\n",
    "    # compute the highest vegetation point in each grid cell\n",
    "    \n",
    "    veg_returns = np.where(np.logical_or(points[\"Classification\"].values == 3,\n",
    "                             points[\"Classification\"].values == 4,\n",
    "                             points[\"Classification\"].values == 5)) \n",
    "\n",
    "    vegpoints = points[\"HeightAboveGround\"].values[veg_returns]\n",
    "\n",
    "    cth = np.max(vegpoints)\n",
    "    \n",
    "    return(cth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_cth1(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a dataframe for pretty querying purposes.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#points = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the points that intersect with each subpolygon and add them to points_within_geometry\n",
    "points_within_geometry = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_points(poly, df, sindex):\n",
    "    poly = poly.buffer(1e-14).buffer(0)\n",
    "    possible_matches_index = list(sindex.intersection(poly.bounds))\n",
    "    possible_matches = df.iloc[possible_matches_index]\n",
    "    precise_matches = possible_matches[possible_matches.intersects(poly)]\n",
    "    \n",
    "    return(precise_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyX, polyY = poly.centroid.xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyBaseXCoord = polyX[0] - metadata[\"metadata\"][\"readers.las\"][0][\"minx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayXindex = (polyBaseXCoord / (resolution/2 )) -1\n",
    "arrayXindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyBaseYCoord = polyY[0] - metadata[\"metadata\"][\"readers.las\"][0][\"miny\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayYindex = (polyBaseYCoord / (resolution/2 )) - 1 \n",
    "arrayYindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayX = PolyBaseCoord - numberofcells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyY[0] - metadata[\"metadata\"][\"readers.las\"][0][\"miny\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tilewidth = metadata[\"metadata\"][\"readers.las\"][0][\"maxx\"] - metadata[\"metadata\"][\"readers.las\"][0][\"minx\"]\n",
    "tilewidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tileheight = metadata[\"metadata\"][\"readers.las\"][0][\"maxy\"] - metadata[\"metadata\"][\"readers.las\"][0][\"miny\"]\n",
    "tileheight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcfRaster = np.zeros((tilewidth, tileheight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcfRaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "matches = get_points(poly, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity, returnnumber, numberofreturns, elevation, hag  = extract_vars(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.size(np.where(matches[\"NumberOfReturns\"].values == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OK now we can make magic - extracting each grid cell, we can rasterify it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf = comp_vcf(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcfRaster[0,138] = vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcfRaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this = matches[\"Classification\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg = np.where(np.logical_or(matches[\"Classification\"].values == 3,matches[\"Classification\"].values == 4, matches[\"Classification\"].values == 5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this = matches[\"Classification\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(matches[\"HeightAboveGround\"].values[veg] < 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcf0105 = comp_lcf(matches, [1, 5], vcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code purgatory\n",
    "stuff here might be useful, or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will likely evolve to take a 'what to grid' input\n",
    "# actually likely not needed...\n",
    "def grid_setup(pointmetadata, resolution):\n",
    "    \"\"\"\n",
    "    Sets up array indexes for an incoming las file, using an input resolution\n",
    "    \n",
    "    input:\n",
    "    - las file metadata\n",
    "    - a scalar resolution\n",
    "    \n",
    "    output:\n",
    "    - a numpy array of indexing values to divide the input points into\n",
    "      'resolution' x 'resolution' bins.\n",
    "      \n",
    "    implicit assumptions:\n",
    "    - 'resolution' is always set in native LAS file units\n",
    "    \"\"\"\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not used yet - geopandas is doing this part!\n",
    "def create_spatial_index(arrays):\n",
    "    \"\"\"\n",
    "    task here is to map geospatial space to numpy array space.\n",
    "    \n",
    "    - using the point metadata set up a grid of [resolution] x [resolution]\n",
    "    - scan the point coordinates array to see which points live in which cell\n",
    "    - create an index which maps point array indexes to grid indexes\n",
    "    \n",
    "    idly wondering if a fast point-in-polygon does this job. Shapely / OGR to the rescue?\n",
    "    ...this would be parallelisable...\n",
    "    \"\"\"\n",
    "    points = []\n",
    "    \n",
    "    for thepoint in arrays[0]:\n",
    "        #print(thepoint[0])\n",
    "        points.append(Point(thepoint[0], thepoint[1]))\n",
    "    \n",
    "    tree = STRtree(points)\n",
    "    \n",
    "    return(points, tree)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
